### 1. requirements
- å¯ä»¥ç§‘å­¦ä¸Šç½‘

### 2. å¦‚ä½•è®¡ç®—æ¨¡å‹éœ€è¦å¤šå°‘GPUå†…å­˜
```math
M = (P * 4B)/(32/Q) * 1.2
```

| Field | explain                                  |
| ----- | ---------------------------------------- |
| M     | GPUå†…å­˜(ä»¥Gigabyteä¸ºå•ä½)                |
| P     | æ¨¡å‹ä¸­çš„å‚æ•°æ•°é‡ã€‚egï¼Œ7Bæ¨¡å‹æœ‰70äº¿ä¸ªå‚æ•° |
| 4B    | 4ä¸ªå­—èŠ‚ï¼Œè¡¨ç¤ºæ¯ä¸ªå‚æ•°ä½¿ç”¨çš„å­—èŠ‚æ•°        |
| 32    | æ¯ä¸ªå­—èŠ‚æœ‰32(bit)                        |
| Q     | åŠ è½½æ¨¡å‹æ—¶ä½¿ç”¨çš„ä½æ•°ï¼Œ4ï¼Œ8æˆ–è€…16ä½       |
| 1.2   | è¡¨ç¤ºå†…å­˜åŠ è½½è¿‡ç¨‹ä¸­é¢å¤–çš„æ¶ˆè€—             |

### 3. llamafactoryè®­ç»ƒæµç¨‹
- å®¿ä¸»æœºæ“ä½œç³»ç»Ÿ
  - Anolis8.9

    Centos8 or redhat8

- ç‰©ç†æœºå®‰è£…nvidiaé©±åŠ¨å’Œcudaç¯å¢ƒ

  - å®‰è£…podman
  
    ä½¿ç”¨podmanä¸æ¨èdockeråŸå› , podmané‡‡ç”¨CDIæ”¯æŒå®¹å™¨è®¿é—®nvidiaçš„GPUé©±åŠ¨,
    è€Œdockerè¿˜æ˜¯é‡‡ç”¨è€æ—§çš„running timeæ¶æ„è®¿é—®å®¿ä¸»æœºgpué©±åŠ¨.
 
    ```bash
    dnf install -y docker
    systemctl start podman
    systemctl enable podman
    ```

  - å®‰è£…nvidiaé©±åŠ¨å’Œcudaç¯å¢ƒ
  
    - ä¸‹è½½nvidiaé©±åŠ¨å’Œcudaåˆ°nvidiaç›®å½•ä¸‹

    ```bash
    mkdir nvidia
    wget https://cn.download.nvidia.com/XFree86/Linux-x86_64/550.107.02/NVIDIA-Linux-x86_64-550.107.02.run
    wget https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda_12.4.0_550.54.14_linux.run
    ```
 
    - å®‰è£…nvidiaé©±åŠ¨å’Œcuda

    ```bash
    bash NVIDIA-Linux-x86_64-550.107.02.run
    bash cuda_12.4.0_550.54.14_linux.run
  
    # é…ç½®rootç¯å¢ƒå˜é‡è®¿é—®cuda
    echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc
    echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
    source ~/.bashrc
    ```
  
    - éªŒè¯nvidiaé©±åŠ¨å’Œcuda

    ```bash
    nvidia-smi
    nvcc --version
    ```

  - å®‰è£…nvidia CDIç¯å¢ƒ

    é»˜è®¤podmanä¸æ”¯æŒç›´æ¥è®¿é—®ç‰©ç†æœºè®¾å¤‡, äº‹å®ä¸Šä¹Ÿæ˜¯æ”¯æŒçš„, åªä¸è¿‡ä½ éœ€è¦æ‰‹åŠ¨æŒ‡å®šæ‰€æœ‰åº“å’Œdevice.
    ä¸ºäº†æ–¹ä¾¿æˆ‘ä»¬ç›´æ¥ä½¿ç”¨CDIæ ‡å‡†, è®©å®¹å™¨æ¥é€šè¿‡é…ç½®æ–‡ä»¶è®¿é—®ç‰©ç†æœºçš„device.
 
    - é…ç½®dnfåº“
    
    ```bash
    curl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo | \
    tee /etc/yum.repos.d/nvidia-container-toolkit.repo
    ```

    - å®‰è£…nvidia-container-toolkit

    ```bash
    yum-config-manager --enable nvidia-container-toolkit-experimental
    dnf install -y nvidia-container-toolkit
    ```

    - å®¿ä¸»æœºä¸­ç”ŸæˆCDIè®¾å¤‡
  
    ```bash
     nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml
    ```

    - æ£€æŸ¥ç”Ÿæˆçš„è®¾å¤‡çš„åç§°
  
    ```bash
    nvidia-ctk cdi list
    ```

    - æµ‹è¯•æ˜¯å¦å¯ä»¥åœ¨å®¹å™¨å†…è®¿é—®å®¿ä¸»æœºçš„nvidiaé©±åŠ¨

    ```bash
    podman run --rm --device nvidia.com/gpu=all --security-opt=label=disable ubuntu nvidia-smi
    ```

  - æ ¹æ®nvidiaåŸºç¡€é•œåƒåˆ¶ä½œimage

    dockeré•œåƒéœ€è¦å®‰è£…llamafactoryç¯å¢ƒ

    dockeré•œåƒä¸­åªéœ€è¦å®‰è£…cudaç¯å¢ƒå³å¯ï¼Œè¦æ±‚<=ç‰©ç†æœºçš„cuda

    docker buildç¼–è¯‘LLammfactoryé•œåƒ
    äº‹å®ä¸Š, å¼ºçƒˆå»ºè®®ä½¿ç”¨nvidiaçš„åŸºç¡€é•œåƒ, åœ¨ä¸Šé¢ä¿®æ”¹é•œåƒ
    è¿›å…¥dockerç›®å½•, æ‰§è¡Œ

    ```bash
    docker build -t alexan/llamafactory .
    ```

  - æµ‹è¯•æ˜¯å¦åœ¨dockerä¸­å¯ä»¥ä½¿ç”¨gpu

    å®¹å™¨å†…

    ```python
    import torch
    torch.cuda.current_device()
    torch.cuda.get_device_name(0)
    torch.__version__
    ```

    å®¹å™¨å¤–

    ```bash
    podman run --rm --device nvidia.com/gpu=all localhost/alexan/llamafactory:latest nvidia-smi
    ```

  - podmanè¿è¡Œgpuå®¹å™¨

  ```bash
  # æŠŠå¤–éƒ¨æºç ç›®å½•æ˜ å°„åˆ°å®¹å™¨å†…
  podman run -ti --device nvidia.com/gpu=all --network host -v /home/root/prog/llvm/:/opt/llvm localhost/alexan/llamafactory:latest bash
  ```

  - å®¿ä¸»æœºé¡¹ç›®ç›®å½•ä»‹ç»
  
  ![å®¿ä¸»æœºç›®å½•](images/dir.png)

    - docker

      å­˜æ”¾Dockerfileæ–‡ä»¶
  
    - src/deply
    
      å­˜æ”¾æ¨¡å‹çš„modefileæ–‡ä»¶
 
    - src/models

      å­˜æ”¾ç”Ÿæˆçš„æ¨¡å‹

    - src/sources

      å­˜æ”¾llama.cpp, ç”¨äºè¿›è¡Œæ¨¡å‹åˆå¹¶çš„æºç 

    - src/test

      å®¹å™¨å†…æµ‹è¯•cudaçš„è„šæœ¬(è¿™é‡Œåå­—å†™é”™äº†)
  

- ä¸‹é¢éƒ½æ˜¯å®¹å™¨å†…æ“ä½œ

- é…ç½®LLamafactoryç¯å¢ƒå’Œè®­ç»ƒæ¨¡å‹

  - å®‰è£…ç¯å¢ƒ
    å¦‚æœé•œåƒåˆ¶ä½œå¥½äº†, è¿™é‡Œå¯ä»¥skip.    

    ```bash
    git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
    cd LLaMA-Factory
    pip install -e ".[torch,metrics]"
    ```

  - æ³¨å†Œæ•°æ®é›†

    è¿™é‡Œå»ºè®®ä½¿ç”¨python3å¤„ç†æ•°æ®

    - åœ¨LLamafactory/dataç›®å½•ä¸‹åˆ›å»ºæ•°æ®é›†

      è¦ç¬¦åˆæ ¼å¼alexan_en_demo.json

      ```bash
      [
        {
          "instruction": "Describe a process of making crepes.",
          "input": "",
          "output": "Making crepes is an easy and delicious process! Here are step-by-step instructions on how to make them:\n\n1. Assemble your ingredients. For basic crepes, you'll need: 1 cup all-purpose flour, 2 eggs, 1/2 cup milk, 1/2 cup water, 1/4 teaspoon salt, and 2 tablespoons melted butter.\n\n2. Mix the batter: In a large mixing bowl, whisk together the flour and the eggs. Gradually add the milk and water, stirring constantly to ensure that there are no lumps. Add salt and melted butter, and mix well.\n\n3. Let the batter rest: If you can, let the batter sit for an hour or so. This will help the flour to absorb the liquid and make the crepes more tender.\n\n4. Heat your pan: Preheat a non-stick pan over medium heat. Lightly butter the pan or use cooking spray to prevent the crepes from sticking.\n\n5. Pour the batter: Using a ladle or a measuring cup, pour a small amount of batter (about 1/4 cup) onto the center of the pan. Immediately tilt the pan in a circular motion to spread the batter evenly and thinly over the bottom of the pan.\n\n6. Cook the crepe: Cook the crepe for 1-2 minutes until the bottom is lightly golden. Carefully loosen the edges with a spatula and flip the crepe over to cook the other side for another minute.\n\n7. Remove and repeat: Gently slide the crepe onto a plate, and then repeat the process with the remaining batter. Remember to re-butter the pan between each crepe if necessary.\n\n8. Fill and serve: Fill your cooked crepes with your desired filling, such as fresh fruit, whipped cream, Nutella, or ham and cheese. Roll or fold, and serve immediately. Enjoy!"
        },
        {
          "instruction": "Transform the following sentence using a synonym: The car sped quickly.",
          "input": "",
          "output": "The car accelerated rapidly."
        },
        {
          "instruction": "Make a persuasive argument to promote recycling.",
          "input": "",
          "output": "Recycling is an incredibly important action that we must all take to help preserve our planet for future generations. By recycling, we can reduce the amount of waste that goes to landfills, conserve natural resources and energy, and reduce pollution from production of new materials.\n\nWhen we don't recycle, our garbage ends up in landfills where it can take hundreds of years to decompose. Landfills can release harmful chemicals and greenhouse gases, polluting the soil, air and water. However, by recycling and reusing materials, we minimize the amount of garbage that ends up in landfills, and therefore reduce the potential harm it can cause to the environment.\n\nRecycling also helps conserve our natural resources by reducing the demand for new materials. For example, recycling paper means fewer trees need to be cut down, and reusing metal and plastic means less mining and oil extraction. This, in turn, conserves energy, minimizes deforestation and reduces the environmental impact associated with producing new materials.\n\nAdditionally, recycling also has a positive impact on the economy. It creates jobs in the recycling and manufacturing industries, and reduces the costs of waste disposal for companies and taxpayers. Recycling is good for the environment and the economy.\n\nIn conclusion, recycling is an easy and effective way to take action and make a difference for the future of our planet. By simply separating our recyclable materials and disposing of them properly, we can help reduce waste, conserve resources, and protect the environment. We all have a part to play and every little action counts. So, let's make the smart choice and recycle today for a better tomorrow."
        }
      ]
      ```

    - ç¼–è¾‘llamafactory/dataç›®å½•ä¸‹çš„dataset_info.jsonæ–‡ä»¶

      è¿½åŠ æ³¨å†Œçš„æ–‡ä»¶

      ```bash
      "alexan_en_demo": {
      "file_name": "alexan_en_demo.json"
      },
      ```

  - é€‰æ‹©æ¨¡å‹

  - å¾®è°ƒè®­ç»ƒ(lora)

    webuiæ“ä½œ

  - æ¨¡å‹å¯¼å‡º

    webuiæ“ä½œï¼Œéœ€è¦ç»‘å®šè®­ç»ƒåçš„ç»“æœ

  - åˆå¹¶æ¨¡å‹

    ```bash
    git clone https://github.com/ggerganov/llama.cpp.git
    
    # åˆå¹¶æ¨¡å‹
    python3 convert_hf_to_gguf.py --outfile /opt/llvm/src/models/alexan-0.5b/alexan-0.5b.gguf /opt/llvm/src/models/alexan-0.5b/
    
    # llamafactory-cli export merge_config.yaml
    ```

  - é‡åŒ–æ¨¡å‹

    è¿›å…¥llama.cppç›®å½•

    ```bash
    mkdir build
    cmake ..
    cmake --build . --config Release
    ```

    ç„¶åæ‰§è¡Œé‡åŒ–æ“ä½œ

    ```bash
    
    ```

- ollamaéƒ¨ç½²æ¨¡å‹

  å®‰è£…ollama, å¦‚æœå®¹å™¨åˆ¶ä½œå¥½äº†, å¯ä»¥skip

  ```bash
  curl -fsSL https://ollama.com/install.sh | sh
  ```

  å¯åŠ¨ollama serve

  ```bash
  ollama serve
  ```

  æ³¨å†Œæ¨¡å‹

  - ç¼–è¾‘alexan-0.5b.modelfileæ–‡ä»¶

    ```bash
    FROM /opt/llvm/src/models/alexan-0.5b/alexan-0.5b.gguf
    ```

  - æ³¨å†Œæ¨¡å‹

    ```bash
    ollama create alexan-0.5b -f alexan-0.5b.modelfile
    ```

  éƒ¨ç½²æ¨¡å‹

  ```bash
  ollama run alexan-0.5b:latest
  ```

- openwebuiæ˜¾ç¤ºwebæ¨¡å‹

  æœ€å¥½ä½¿ç”¨å¦ä¸€ä¸ªå®¹å™¨

  ```bash
  docker pull swr.cn-north-4.myhuaweicloud.com/ddn-k8s/ghcr.io/open-webui/open-webui:v0.3.12
  docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/opt/llvm/app/backend/data --name open-webui swr.cn-north-4.myhuaweicloud.com/ddn-k8s/ghcr.io/open-webui/open-webui:v0.3.12
  ```

### 3. refer

[nvidia install cdi](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-yum-or-dnf)

ğŸ“LicenseğŸ’–

